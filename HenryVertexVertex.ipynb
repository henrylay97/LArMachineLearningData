{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Setup!\n",
    "isWeighted = False\n",
    "nTrees     = 100\n",
    "maxDepth   = 2\n",
    "\n",
    "filtered      = False\n",
    "plotVariables = True\n",
    "\n",
    "version  = \"All\"\n",
    "removals = {*range(0,92),*range(93,95)}\n",
    "\n",
    "#version  = \"Original\"\n",
    "#removals = {*range(7,21),*range(26,55),*range(61,90),*range(91,95)}\n",
    "\n",
    "#version  = \"NewEventShapes\"\n",
    "#removals = {*range(2,4),*range(7,9),*range(11,21),*range(26,55),\n",
    "#            *range(61,90),*range(91,95)}\n",
    "\n",
    "#version  = \"Smooth\"\n",
    "#removals = {*range(7,20),*range(22,25),*range(26,30),*range(32,52),\n",
    "#            *range(55,58),*range(59,63),*range(65,85)}\n",
    "\n",
    "#version  = \"EnergyVars\"\n",
    "#removals = {*range(7,20),*range(25,32),*range(33,38),*range(39,45),\n",
    "#            *range(46,50),*range(51,52),*range(58,65),*range(66,71),\n",
    "#            *range(72,78),*range(79,83),*range(84,85)}\n",
    "\n",
    "#version  = \"NewEventVars\"\n",
    "#removals = {*range(7,16),*range(17,18),*range(25,52),*range(58,85)}\n",
    "\n",
    "#version  = \"Basic\"\n",
    "#removals = {*range(1,21),*range(24,55),*range(59,90),*range(91,95)}\n",
    "\n",
    "#version  = \"BasicPlus\"\n",
    "#removals = {*range(2,4),*range(5,6),*range(7,21),*range(25,55),\n",
    "#            *range(60,90),*range(91,95)}\n",
    "\n",
    "#version  = \"BasicAndEnergy\"\n",
    "#removals = {*range(1,20),*range(23,32),*range(33,45),*range(46,50),\n",
    "#            *range(51,52),*range(56,65),*range(66,78),*range(79,83),\n",
    "#            *range(84,85)}\n",
    "\n",
    "#version  = \"BasicPlusAndEnergy\"\n",
    "#removals = {*range(2,4),*range(5,6), *range(7,20),*range(24,32),\n",
    "#            *range(33,45),*range(46,50),*range(51,52),*range(57,65),\n",
    "#            *range(66,78),*range(79,83),*range(84,85)}\n",
    "\n",
    "#version  = \"EnergyVarsSmoothdEdx\"\n",
    "#removals = {*range(7,20),*range(25,33),*range(34,38),*range(39,45),\n",
    "#            *range(46,50),*range(51,52),*range(58,66),*range(67,71),\n",
    "#            *range(72,78),*range(79,83),*range(84,85)}\n",
    "\n",
    "#version  = \"Linking\"\n",
    "#removals = {*range(7,21),*range(26,55),*range(61,90)}\n",
    "\n",
    "#version  = \"Optimum1\"\n",
    "#removals = {*range(2,4),*range(5,6),*range(7,21),*range(25,33),\n",
    "#            *range(34,45),*range(46,55),*range(60,68),*range(69,80),\n",
    "#            *range(81,90),*range(93,94)}\n",
    "\n",
    "#version  = \"Optimum2\"\n",
    "#removals = {*range(2,4),*range(5,6),*range(7,21),*range(25,33),\n",
    "#            *range(34,45),*range(46,54),*range(60,68),*range(69,80),\n",
    "#            *range(81,89),*range(93,94)}\n",
    "\n",
    "#version  = \"OptimumSmooth\"\n",
    "#removals = {*range(2,4),*range(5,6),*range(7,21),*range(23,26),\n",
    "#            *range(27,31),*range(32,34),*range(35,45),*range(46,55),\n",
    "#            *range(58,61),*range(62,66),*range(67,69),*range(70,80),\n",
    "#            *range(81,90),*range(93,94)}\n",
    "\n",
    "#version  = \"OptimumMC\"\n",
    "#removals = {*range(2,4),*range(5,6),*range(7,21),*range(23,24),\n",
    "#            *range(25,29),*range(30,37),*range(38,45),*range(46,55),\n",
    "#            *range(58,59),*range(60,64),*range(65,72),*range(73,80),\n",
    "#            *range(81,90),*range(93,94)}\n",
    "\n",
    "#version  = \"OptimumStripped\"\n",
    "#removals = {*range(0,6),*range(7,21),*range(25,33),*range(34,45),\n",
    "#            *range(46,55),*range(60,68),*range(69,80),*range(81,90),\n",
    "#            *range(93,95)}\n",
    "\n",
    "#version  = \"OptimumStrippedPlusSharedEnergy\"\n",
    "#removals = {*range(0,6),*range(7,21),*range(25,33),*range(34,45),\n",
    "#            *range(46,55),*range(60,68),*range(69,80),*range(81,90),\n",
    "#            *range(94,95)}\n",
    "\n",
    "#version  = \"OptimumMinusLinking\"\n",
    "#removals = {*range(2,4),*range(5,6),*range(7,21),*range(25,33),\n",
    "#            *range(34,45),*range(46,55),*range(60,68),*range(69,80),\n",
    "#            *range(81,90),*range(91,95)}\n",
    "\n",
    "#version  = \"OptimumStrippedSmoothdEdx\"\n",
    "#removals = {*range(0,6),*range(7,21),*range(25,34),*range(35,45),\n",
    "#            *range(46,55),*range(60,69),*range(70,80),*range(81,90),\n",
    "#            *range(93,95)}\n",
    "\n",
    "#version  = \"NoBeamDW\"\n",
    "#removals = {*range(7,22),*range(26,55),*range(56,57),*range(61,90),*range(91,95)}\n",
    "\n",
    "#version  = \"Final\"\n",
    "#removals = {*range(2,4),*range(7,9),*range(11,21),*range(26,33),\n",
    "#            *range(34,45),*range(46,55),*range(61,68),*range(69,80),\n",
    "#            *range(81,90),*range(93,95)}\n",
    "\n",
    "if isWeighted: version += \"_Weighted\"\n",
    "if filtered  : version += \"_Filtered\"\n",
    "version += (\"_\" + str(nTrees) + \"_\" + str(maxDepth))\n",
    "\n",
    "print(version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup notifications\n",
    "from time import time as tme\n",
    "from IPython import get_ipython\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "class Beeper:\n",
    "\n",
    "    def __init__(self, threshold, **audio_kwargs):\n",
    "        self.threshold = threshold\n",
    "        self.start_time = None    # time in sec, or None\n",
    "        self.audio = audio_kwargs\n",
    "\n",
    "    def pre_execute(self):\n",
    "        if not self.start_time:\n",
    "            self.start_time = tme()\n",
    "\n",
    "    def post_execute(self):\n",
    "        end_time = tme()\n",
    "        if self.start_time and end_time - self.start_time > self.threshold:\n",
    "            audio = Audio(**self.audio, autoplay=True)\n",
    "            display(audio)\n",
    "        self.start_time = None\n",
    "\n",
    "beeper = Beeper(5, filename='/usr/share/sounds/gnome/default/alerts/drip.ogg')\n",
    "\n",
    "ipython = get_ipython()\n",
    "ipython.events.register('pre_execute', beeper.pre_execute)\n",
    "ipython.events.register('post_execute', beeper.post_execute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the relevant scripts from LArMachineLearningData\n",
    "# Nice the process so it can run with lots of cores on low priority\n",
    "import os\n",
    "os.nice(20)\n",
    "\n",
    "# Add path for LArMachineLearningData\n",
    "import sys\n",
    "pandoraMVADir = os.environ['PANDORAMVADIR']\n",
    "dataDir       = os.environ['PANDORABDTDATADIR']\n",
    "\n",
    "sys.path.append(pandoraMVADir + 'LArMachineLearningData/scripts')\n",
    "\n",
    "# Import pandora libraries\n",
    "from importlib import reload\n",
    "from PandoraBDT import *\n",
    "\n",
    "# Import concatenation tool\n",
    "from itertools import chain\n",
    "\n",
    "# Import relevant SKLearn libraries\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn import metrics\n",
    "\n",
    "# Set global params\n",
    "testTrainFraction = 0.5\n",
    "nCores = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data and Name\n",
    "trainingFile = dataDir + 'training_file_vertex_mix_new.txt'\n",
    "BDTName = \"VertexVertex\"\n",
    "\n",
    "# Directories\n",
    "plotsDir = pandoraMVADir + 'bdt/plots/' + BDTName + '/' + version + '/'\n",
    "saveDir  = pandoraMVADir + 'bdt/trained/' + BDTName + '/' + version + '/'\n",
    "print(plotsDir)\n",
    "print(saveDir)\n",
    "\n",
    "if not os.path.exists(plotsDir):\n",
    "    os.makedirs(plotsDir)\n",
    "    \n",
    "if not os.path.exists(saveDir):\n",
    "    os.makedirs(saveDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some analysis specific things\n",
    "allFeatureNames = ['Event_Showeryness', # 0\n",
    "                   'Event_Energy',\n",
    "                   'Event_Volume',\n",
    "                   'Event_Longitudinality',\n",
    "                   'Event_Num_Hits', \n",
    "                   'Event_Num_Clusters', # 5\n",
    "                   'Event_Num_Candidates', \n",
    "                   \n",
    "                   'Event_Area', # 7\n",
    "                   'Event_Longitudinality2D',\n",
    "                   'Event_Area_Average_Z',\n",
    "                   'Event_Longitudinality2D_Average_Z', #10\n",
    "                   'Event_Hit_Proportion',\n",
    "                   'Event_Energy_Proportion',\n",
    "                   'Event_Hit_Outlier_Proportion',\n",
    "                   'Event_Energy_Outlier_Proportion',\n",
    "                   'Event_Hit_Harsh_Proportion', # 15\n",
    "                   'Event_Energy_Harsh_Proportion',\n",
    "                   'Event_Hit_Outlier_Harsh_Proportion',\n",
    "                   'Event_Energy_Outlier_Harsh_Proportion',\n",
    "                   'Event_Num_Regions',\n",
    "                   'Event_Num_Region_Candidates', # 20\n",
    "\n",
    "                   'Vertex1_Beam_Deweighting', # 21\n",
    "                   'Vertex1_Energy_Kick',\n",
    "                   'Vertex1_Global_Asymmetry', \n",
    "                   'Vertex1_Local_Asymmetry',\n",
    "                   'Vertex1_Shower_Asymmetry', # 25\n",
    "\n",
    "                   'Vertex1_Global_Smooth_Asymmetry', # 26\n",
    "                   'Vertex1_Global_Asymmetry_AC',\n",
    "                   'Vertex1_Global_Smooth_Asymmetry_AC',\n",
    "                   'Vertex1_Global_Asymmetry_MC',\n",
    "                   'Vertex1_Global_Smooth_Asymmetry_MC', # 30\n",
    "                   'Vertex1_Local_Smooth_Asymmetry',\n",
    "                   'Vertex1_Shower_Smooth_Asymmetry',\n",
    "                   'Vertex1_dEdx_Asymmetry',\n",
    "                   'Vertex1_dEdx_Smooth_Asymmetry',\n",
    "                   'Vertex1_dEdx_Asymmetry_AC', # 35\n",
    "                   'Vertex1_dEdx_Smooth_Asymmetry_AC',\n",
    "                   'Vertex1_dEdx_Asymmetry_MC',\n",
    "                   'Vertex1_dEdx_Smooth_Asymmetry_MC',\n",
    "                   'Vertex1_dEdx_Deviation',\n",
    "                   'Vertex1_dEdx_Deviation_AC', # 40\n",
    "                   'Vertex1_dEdx_Deviation_MC',\n",
    "                   'Vertex1_Braggness',\n",
    "                   'Vertex1_Braggness_AC',\n",
    "                   'Vertex1_Braggness_MC',\n",
    "                   'Vertex1_Energy', # 45\n",
    "                   'Vertex1_Average_Energy',\n",
    "                   'Vertex1_Charge',\n",
    "                   'Vertex1_Average_Charge',\n",
    "                   'Vertex1_Energy_Ratio',\n",
    "                   'Vertex1_Charge_Ratio', # 50\n",
    "                   'Vertex1_Num_Local_Clusters',\n",
    "                   'Vertex1_Num_Local_Sliding_Fits',\n",
    "                   'Vertex1_Fit_Cluster_Ratio',\n",
    "                   'Vertex1_Mean_Scatter',\n",
    "                   'Vertex1_rPhi', # 55\n",
    "\n",
    "                   'Vertex2_Beam_Deweighting', # 56\n",
    "                   'Vertex2_Energy_Kick',\n",
    "                   'Vertex2_Global_Asymmetry',\n",
    "                   'Vertex2_Local_Asymmetry',\n",
    "                   'Vertex2_Shower_Asymmetry', # 60\n",
    "\n",
    "                   'Vertex2_Global_Smooth_Asymmetry', # 61\n",
    "                   'Vertex2_Global_Asymmetry_AC',\n",
    "                   'Vertex2_Global_Smooth_Asymmetry_AC',\n",
    "                   'Vertex2_Global_Asymmetry_MC',\n",
    "                   'Vertex2_Global_Smooth_Asymmetry_MC', # 65\n",
    "                   'Vertex2_Local_Smooth_Asymmetry',\n",
    "                   'Vertex2_Shower_Smooth_Asymmetry',\n",
    "                   'Vertex2_dEdx_Asymmetry',\n",
    "                   'Vertex2_dEdx_Smooth_Asymmetry',\n",
    "                   'Vertex2_dEdx_Asymmetry_AC', # 70\n",
    "                   'Vertex2_dEdx_Smooth_Asymmetry_AC',\n",
    "                   'Vertex2_dEdx_Asymmetry_MC',\n",
    "                   'Vertex2_dEdx_Smooth_Asymmetry_MC',\n",
    "                   'Vertex2_dEdx_Deviation',\n",
    "                   'Vertex2_dEdx_Deviation_AC', # 75\n",
    "                   'Vertex2_dEdx_Deviation_MC',\n",
    "                   'Vertex2_Braggness',\n",
    "                   'Vertex2_Braggness_AC',\n",
    "                   'Vertex2_Braggness_MC',\n",
    "                   'Vertex2_Energy', # 80\n",
    "                   'Vertex2_Average_Energy',\n",
    "                   'Vertex2_Charge',\n",
    "                   'Vertex2_Average_Charge',\n",
    "                   'Vertex2_Energy_Ratio',\n",
    "                   'Vertex2_Charge_Ratio', # 85\n",
    "                   'Vertex2_Num_Local_Clusters',\n",
    "                   'Vertex2_Num_Local_Sliding_Fits',\n",
    "                   'Vertex2_Fit_Cluster_Ratio',\n",
    "                   'Vertex2_Mean_Scatter',\n",
    "                   'Vertex2_rPhi', # 90\n",
    "                   \n",
    "                   'Shared_Separation', # 91\n",
    "                   'Shared_Axis_Hits',\n",
    "                   'Shared_Axis_Energy',\n",
    "                   'Shared_Axis_Ratio'\n",
    "                  ]\n",
    "\n",
    "# Set background and signal label names\n",
    "params = {\n",
    "    'labelNames': ['Incorrect Vertex','Correct Vertex'],\n",
    "    'signalDefs': [0, 1],\n",
    "    'signalCols': ['r', 'b']\n",
    "}\n",
    "\n",
    "# Create the base BDT to vary the params from and compare to\n",
    "baseBDT = AdaBoostClassifier(DecisionTreeClassifier(max_depth=maxDepth),algorithm='SAMME', \n",
    "                             random_state=42, n_estimators=nTrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "allData, nFeatures, nExamples = LoadData(trainingFile, ',')\n",
    "\n",
    "# Take weights before removing variables\n",
    "weightsOrg = GetWeights(allData,20)\n",
    "\n",
    "# Get vetos before removing variables\n",
    "vetosOrg   = GetVetos(allData,91,0.3)\n",
    "\n",
    "# Remove unwanted features\n",
    "data         = RemoveFeature(allData,removals)\n",
    "featureNames = []\n",
    "\n",
    "for i in [x for x in range(0,nFeatures) if x not in removals]:\n",
    "    featureNames.append(allFeatureNames[i])\n",
    "\n",
    "del allData, allFeatureNames\n",
    "\n",
    "# Redefine size\n",
    "nFeatures = len(data[0]) - 1\n",
    "\n",
    "# Check removals\n",
    "print(nFeatures, \" =? \", len(featureNames))\n",
    "print(featureNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresOrg, labelsOrg           = SplitTrainingSet(data, nFeatures)\n",
    "features, labels, weights, vetos = Randomize(featuresOrg, labelsOrg, weightsOrg, vetosOrg, True)\n",
    "\n",
    "# Split into train and test samples\n",
    "xUnfTrain, yUnfTrain, weightsUnfTrain, vetosTrain, xTest, yTest, weightsTest = Sample(features, labels, weights, vetos, testTrainFraction)\n",
    "\n",
    "if filtered :\n",
    "    xTrain, yTrain, weightsTrain = Filter(xUnfTrain,yUnfTrain,weightsUnfTrain, vetosTrain)\n",
    "else :\n",
    "    xTrain       = xUnfTrain\n",
    "    yTrain       = yUnfTrain\n",
    "    weightsTrain = weightsUnfTrain\n",
    "    \n",
    "# Split into signal and background based on the true labels\n",
    "signalFeatures = features[labels==1]\n",
    "backgroundFeatures = features[labels==0]\n",
    "\n",
    "# Check the features array is the same size as the feature names array\n",
    "print('Number of features: ' + str(len(featureNames)))\n",
    "print('Shape of data array: ' + str(np.shape(features)))\n",
    "print('Total: ' + str(len(features)) + ', signal: ' +\n",
    "      str(len(signalFeatures)) + ' and background: ' +\n",
    "      str(len(backgroundFeatures)))\n",
    "print(len(weights))\n",
    "print(len(vetos))\n",
    "print(str(len(xTrain)) + ' vs. ' + str(len(xTest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the Pandas dataframe\n",
    "# First create a dictionary\n",
    "allDict = {featureNames[i]: features[:, i] for i in range(nFeatures)}\n",
    "allDict.update({'Labels': labels})\n",
    "\n",
    "# Create the Pandas dataframe, create seperate df for signal/background\n",
    "df = pd.DataFrame(data=allDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make plots drawing the variables for signal/background\n",
    "if plotVariables : DrawVariablesDF(df, params, plotsDir, save = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make correlation matricies\n",
    "if plotVariables :\n",
    "    Correlation(df[df['Labels']==params['signalDefs'][0]], params['labelNames'][0] + ' Correlation Matrix',plotsDir, save = True)\n",
    "    Correlation(df[df['Labels']==params['signalDefs'][1]], params['labelNames'][1] + ' Correlation Matrix',plotsDir, save = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference BDT with controlled hyperparams\n",
    "if isWeighted : baseBDT.fit(xTrain,yTrain, sample_weight = weightsTrain)\n",
    "else : baseBDT.fit(xTrain,yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "fig, ax = plt.subplots()\n",
    "metrics.plot_roc_curve(baseBDT, xTest, yTest, ax=ax)\n",
    "\n",
    "plt.title(\"ROC Curves\")\n",
    "ax.invert_xaxis()\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "\n",
    "plt.savefig(plotsDir + '/' + \"roc.png\", bbox_inches='tight')\n",
    "plt.savefig(plotsDir + '/' + \"roc.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Confusion Matricies\n",
    "fig, ax = plt.subplots()\n",
    "metrics.plot_confusion_matrix(baseBDT, xTest, yTest, display_labels=params['labelNames'],\n",
    "                             ax=ax, normalize='true')\n",
    "ax.invert_xaxis()\n",
    "plt.title(\"Confusion matrix (True Normalised)\")\n",
    "\n",
    "plt.savefig(plotsDir + '/' + \"confusion_matrix.png\", bbox_inches='tight')\n",
    "plt.savefig(plotsDir + '/' + \"confusion_matrix.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print more detailed performance info\n",
    "bdtPredicted = baseBDT.predict(xTest)\n",
    "\n",
    "print (\"Background (0): \", params['labelNames'][0])\n",
    "print (\"Signal (1): \", params['labelNames'][1])\n",
    "print (\"BDT:\\n\", metrics.classification_report(yTest, bdtPredicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot importance of features\n",
    "importanceDF = pd.DataFrame({'Features': featureNames, 'Importance Score':baseBDT.feature_importances_})\n",
    "print (importanceDF.sort_values(by=['Importance Score']))\n",
    "ax = importanceDF.sort_values(by=['Importance Score'])\\\n",
    "    .plot(kind='barh', x='Features', y='Importance Score')\n",
    "\n",
    "plt.savefig(plotsDir + '/' + \"feature_importance.png\", bbox_inches='tight')\n",
    "plt.savefig(plotsDir + '/' + \"feature_importance.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PandoraBDT\n",
    "reload (PandoraBDT)\n",
    "from PandoraBDT import *\n",
    "\n",
    "parameters = {\n",
    "  'ClassNames': ['True Vertex', 'Incorrect Vertex'],\n",
    "  'SignalDefinition': [1, 0],\n",
    "  'PlotColors': ['b', 'r'],\n",
    "  'nBins': 100,\n",
    "  'PlotStep': 1.0,\n",
    "  'OptimalBinCut': 0,\n",
    "  'OptimalScoreCut': 0.0,\n",
    "  'nTrees': 100,\n",
    "  'TreeDepth': 3\n",
    "}\n",
    "\n",
    "FindOptimalSignificanceCut(baseBDT, xTest, yTest, parameters)\n",
    "if isWeighted : PlotBdtScoresWeight(baseBDT, xTest, yTest, weightsTest, xTrain, yTrain, 'Vertex Vertex ' + version, parameters, plotsDir, save=True)\n",
    "else : PlotBdtScores(baseBDT, xTest, yTest, xTrain, yTrain, 'Vertex Vertex ' + version, parameters, plotsDir, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WriteXmlFile(saveDir + BDTName + \".xml\", baseBDT, BDTName)\n",
    "SerializeToPkl(saveDir + BDTName + \".pkl\", baseBDT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
